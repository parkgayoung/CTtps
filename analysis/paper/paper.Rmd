---
title: "Title Goes Here"
author:
  -gayoung Park
  - Author Two
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
    bookdown::html_document2:
      fig_caption: yes
      reference_docx: "../templates/template.docx" # Insert path for the DOCX file
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---


<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

```{r, setup, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  echo = FALSE,
  comment = "#>",
  fig.path = "../figures/"
)

#library(CTtps) # Or use devtools::load_all('.', quiet = T) if your code is in script files, rather than as functions in the `/R` diretory

library(here)
library(tidyverse)
```

# Introduction

1. What are our research questions?
- Understanding technological transition during the Late Pleistocene in Korea base on Cultural transmission. 
- What are the social contexts led to the technological transition

2. What is our archaeological background?
-SP was started in Korean Peninsula -> spread to Japan
-Why SP important, Why we care
-Current explantion of SP dispersal: migration vs in situ, in between
 The evidence of the two main models: proportion of SP in assemblage, Y chromosome (two groups of people), composition of assemblage (blade to flake ratio)
 -> no explanation about any mechanism/process/driver for the new technology
 -> lack of details of the lithic technology, human behavior

3. What is our conceptual framework?
- Evolutionary concepts that links stone artifacts to human behavior
- Cultural Evolution : the idea that cultural change constitutes an evolutionary process
- Same general concept of biological evolution: genetic change in population that is inherited over generations. Apply this concept to artifacts or cultural phenomena
- In this paper, I will use Cultural Transmission 
- Cultural Transmssion: way that people learn and pass on information
- Transmission biases
- similar study (CT in stone artifacts) : Mesoudi&O'Brien, Bettinger&Eerkens, Garvey

4. What are our methods?
- Framework: Bettinger&Eerkens
- Two models: Guided variation and Model-based bias =>  Multi-point Origin (in situ or in between one) and Single-point Origin (migration model)
- Test by Correlation and Variation (Pearson's r, PCA, Coefficient of Variation) 
- data collection, sites, artifact #
- attributes: landmarks (tps software)

5. What are our results? 
- overall change through time, grouping (chronology, typology)
- correlation: postively correlated(?)
- PCA: 2,3 phases are highly overlapped, high redundancy. The individual phase are scattered, low redundancy. Variables of Dimension 1 is positively correlated
- CV:values are around 20(ML)-34(SL). Compared to 2nd phase, 3rd phase has slightly lower variation.   

6. What is the answer to our questions??
- low correlation + high variation? High correlation + low variation?
- What are the criteria for correlation and variation
- How was SP spread in Korea, did they have single orgin or multiple origins?

7. What is limitation of this study?
- Maybe applying CV for SP is not ideal. It can be depending on individual knapper's skill and raw materials
- CV of one site (SYG6) - no big difference with the value of all data
- CV: lithis vs pottery

- couldn't include thickness and weight


# Background

```{r demo, eval = FALSE}
1 + 1
```

# Methods

Reading tps data and phase information of each artifact 
```{r}
#reading landmarks in tps, code from https://gist.github.com/mrdwab/2062329

read.tps = function(data) {
  # Reads the .tps file format produced by TPSDIG 
  # (http://life.bio.sunysb.edu/morph/ into a single data frame
  # USAGE: R> read.tps("filename.tps")
  a = readLines(data) # so we can do some searching and indexing
  LM = grep("LM", a) # find the line numbers for LM
  ID.ind = grep("ID", a) # find the line numbers for ID
  # and the ID values, SCALE values, and image names
  ID = gsub("(ID=)(.*)", "\\2", grep("ID", a, value=T)) 
  SCALE = gsub("(SCALE=)(.*)", "\\2", grep("SCALE", a, value=T)) 
  images = basename(gsub("(IMAGE=)(.*)", "\\2", a[ID.ind - 1]))
  # FOR EACH LOOP   
  skip = LM # set how many lines to skip
  # and how many rows to read
  nrows = as.numeric(gsub("(LM=)(.*)", "\\2", grep("LM", a, value=T)))
  l = length(LM) # number of loops we want
  
  landmarks = vector("list", l) # create an empty list
  
  for (i in 1:l) {
    landmarks[i] = list(data.frame(
      read.table(file = data, header = F, skip = LM[i],
                 nrows = nrows[i], col.names = c("X", "Y")),
      IMAGE = as.character(images[i]),
      ID = ID[i],
      SCALE = SCALE[i]))
  }
  do.call(rbind, landmarks) # rbind the list items into a data.frame
}

#need to figure out how to read the file inside the project
landmarks <- read.tps(here("analysis/data/raw_data/A_BG_2.tps"))

#getting info for phase of each site
SPstage <- read.csv(here("analysis/data/SPstage.csv"))

#Compare multiple artifacts 
#Open multiple tps files. This will need to be changed to read files inside the project
datadir <- here("/analysis/data/raw_data/")
files <- dir(datadir, pattern = "*.tps")
```


Computing size attributes of the points by calculating distance between landmakrs
```{r}

#Understanding each landmark
#ML: maximum length, tip to bottom, perpendicular to the length axis 1- 7
#BL: body length: tip to the closest wing, perpendicular to the length axis 1- 3 OR 1-10 depending on the artifact
#TL: Tang length, the closest wing from the tip to bottom, perpendicular to the length axis 9 - 7 depending on the artifact
#SL: max stem length, perpendicular to the length axis. Closer tang curveâ€™s middle point from the tip to the most distant point of the basal end  9-7 depending on the artifact
#MW: mid width: dimension from margin to margin at the mid-point of the length 2 - 11 axis, perpendicular to the width axis.
#TW: tang width: dimension between each wing, perpendicular to the width  axis  3 - 10
#SW: stem width: width of the basal end of the point, 5mm above the end  5 - 8

#distance between landmarks(euclidean)

landmark_dist = function(landmarks, pt1, pt2) {
  sqrt(
    (landmarks$X[pt1] - landmarks$X[pt2])^2 + 
      (landmarks$Y[pt1] - landmarks$Y[pt2])^2
  ) * as.numeric(levels(landmarks$SCALE))
}

# distance between two y axis 
vertical_dist = function(landmarks, pt1, pt2) {
  d = abs(landmarks$Y[pt1] - landmarks$Y[pt2])
  d * as.numeric(levels(landmarks$SCALE))
}


#Maximum length of sp: ML, distance between landmark 1-7
ML <- landmark_dist(landmarks, 1, 7)

#Body length of sp is 1-3 or 1-10, depending on each artifact.
bl_dist = function(landmarks) {
  d1 = landmark_dist(landmarks, 1, 3)
  d2 = landmark_dist(landmarks, 1, 10)
  max(d1, d2)
}

BL <- bl_dist(landmarks)

# Tang length of sp: TL, distance between landmark 10-7 or 3-7, (y axis)
tl_dist <- function(landmarks) {
  t1 = vertical_dist(landmarks, 10, 7)
  t2 = vertical_dist(landmarks, 3, 7)
  max(t1, t2)
}

TL <- tl_dist

# Maximum length of stem: SL, distance between landmark 9-7 or 4-7 (y axis)
sl_dist <- function(landmarks) {
  s1 = vertical_dist(landmarks, 9, 7)
  s2 = vertical_dist(landmarks, 4, 7)
  max(s1, s2)
}

SL <- sl_dist

# Mid width: MW, distance between landmark 2-11
MW <- landmark_dist(landmarks, 2, 11)

# Maximum width of Tang: TW, distance between landmark 3-10
TW <- landmark_dist(landmarks, 3, 10)

# Stem width: SW, distance between landmark 5-8
SW <- landmark_dist(landmarks, 5, 8)

```



Gather attributes from  all artifacts
```{r}

ML = c()
BL = c()
TL = c()
SL = c()
MW = c()
TW = c()
SW = c()
for (i in 1:length(files)) {
  filename <- paste(datadir, files[i], sep = "")
  print(filename)
  landmarks <- read.tps(filename)
  ML <- c(ML, landmark_dist(landmarks, 1, 7))
  BL <- c(BL, bl_dist(landmarks))
  TL <- c(TL, tl_dist(landmarks))
  SL <- c(SL, sl_dist(landmarks))
  MW <- c(MW, landmark_dist(landmarks, 2, 11))
  TW <- c(TW, landmark_dist(landmarks, 3, 10))
  SW <- c(SW, landmark_dist(landmarks, 5, 8))
}

#list of all attributes
df <- data.frame(ML, BL, TL, SL, MW, TW, SW, files, SPstage$Stage)

#trim the name of each artifact.
df_sitename <- df %>% 
  mutate (files = str_remove_all(files, "A_|1|2|.tps")) %>% 
  separate (files, into = c("sitename", "files"), sep = "_") %>% 
  select (-files)

#list of all attributes without ID
DF <- df[,1:7]
```

Box plot for each attributes
```{r}
boxplot(DF, main = "Variaiton for each attributes", col = "gold")
```


Computing Coefficient or variation
```{r}
# Coefficient of variation (CV)

cv <- function(x, ... ) sd(x, ...) / mean(x, ...) *100

# we've got DF a data frame with cols as variables
# rows and specimens, and values as dimensions
# need to add a column of site ID
map(DF, cv)


CV_ML <- cv(ML, na.rm = FALSE) #na.rm: logical. Should missing values be removed?
CV_BL <- cv(BL, na.rm = FALSE)
CV_TL <- cv(TL, na.rm = FALSE)
CV_SL <- cv(SL, na.rm = FALSE)
CV_MW <- cv(MW, na.rm = FALSE)
CV_TW <- cv(TW, na.rm = FALSE)
CV_SW <- cv(SW, na.rm = FALSE)


sd(DF$ML)/mean(DF$ML)*100
sd(BL)/mean(BL)*100
sd(TL)/mean(TL)*100
sd(SL)/mean(SL)*100
sd(MW)/mean(MW)*100
sd(TW)/mean(TW)*100
sd(SW)/mean(SW)*100


```
CV for all attributes are distributed from 23-31%

Make a table of the CV for all variables on all artefacts from all sites

```{r}
# all artefacts from all sites
map_df(DF, cv)
```

Make a table of CVs for all variable grouped by site

```{r}
cv_by_site_df  <- 
df_sitename %>% 
  select(-SPstage.Stage) %>% 
  group_by(sitename) %>% 
  add_tally() %>% 
  filter( n > 1) %>% 
  select(-n) %>% 
  nest(-sitename) %>% 
  mutate(cv_by_site = map(data, ~map_df(.x, cv)))
  
cv_by_site_df_unnest <-   
cv_by_site_df %>% 
  unnest(cv_by_site)
```

Make a table of CVs for all variable grouped by phase
```{r}
cv_by_stage_df  <- 
df_sitename %>% 
  select(-sitename) %>% 
  group_by(SPstage.Stage) %>% 
  add_tally() %>% 
  filter( n > 1) %>% 
  select(-n) %>% 
  nest(-SPstage.Stage) %>% 
  mutate(cv_by_stage = map(data, ~map_df(.x, cv)))
  
cv_by_stage_df_unnest <-   
cv_by_stage_df %>% 
  unnest(cv_by_stage)
```

Computing Pearson's correlation 
```{r}
library(Hmisc)
COR <- rcorr(as.matrix(DF))

#a simple function for formatting a correlation matrix into a table with 4 columns containing from (http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software)
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

flattenCorrMatrix(COR$r, COR$P)

#simpler way to calculate Correlataion, you can use
SCOR <- cor(DF)


#a scatterplot matrix of Correlation
pairs(DF)

# Plot for SCOR
library(corrplot)
corrplot(SCOR, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)

corrplot(COR$r, type="upper", order="hclust", 
         p.mat = COR$P, sig.level = 0.01, insig = "blank")
```

Principal component analysis

Principal Component Analysis is a technique to extract relevant information from a multivariate dataset and to express this information as a set of few new variables called principal components or dimensions [@cascalheira2018use].

In archaeology, PCA has been often used for~

We applied PCA to our dataset to identify correlations between variables that would represent the existence of patterns in the points, which could possilby be translated into more ~ , in addition to correlation analysis.

```{r}
library(ggbiplot)


PCA <- prcomp(DF, center = TRUE, scale = TRUE)

#group them by three phase
library(ggfortify) 

# remove stage 1 artefact
xx_pca <- 
df_sitename %>% 
  filter(SPstage.Stage != 1) %>% 
  select(-sitename, -SPstage.Stage) %>% 
  prcomp(., center = TRUE, scale = TRUE)

xx_pca1 <- 
  data.frame(xx_pca$x,
             stage = as.factor( df_sitename$SPstage.Stage[df_sitename$SPstage.Stage !=1]))

gg_stage <- 
ggplot(xx_pca1 ,
       aes(PC1,
           PC2,
       colour = stage)) +
  geom_point() +
  stat_ellipse()

gg_stage
```

Contribution of variables for each of four PCA demensions
```{r}
library(gridExtra)

PC1_v <- fviz_contrib(xx_pca, choice = "var", axes = 1, top = 7)
PC2_v <- fviz_contrib(xx_pca, choice = "var", axes = 2, top = 7)
PC3_v <- fviz_contrib(xx_pca, choice = "var", axes = 3, top = 7)
PC4_v <- fviz_contrib(xx_pca, choice = "var", axes = 4, top = 7)

grid.arrange(PC1_v, PC2_v, PC3_v, PC4_v, nrow =2)

```

We see some differences in artefact shape and size. Now we need to examins the PCA output numbers to describe in more details. 

```{r}

library(factoextra) #http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/112-pca-principal-component-analysis-essentials/

# Color variables by the continuous variable
fviz_pca_var(xx_pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

```


PCA for 3 factors
```{r}
library(plot3D)

scatter3D(pca_col$PC1, pca_col$PC2, pca_col$PC3, bty = "g", pch = 18, col = gg.col(100), 
          main = "PCA", xlab = "PC1",
          ylab = "PC2", zlab = "PC3")

library(plotly)
plot_ly(pca_col, x = ~PC1, y = ~PC2, z = ~PC3, colors = "Dark2", size = I(120)) %>%  add_markers(color = ~phase)

```








SYG6 site data and analysis

```{r}

# artifacts from SYG6 site
SYG6_files <- dir(datadir, pattern = "SYG6_*")


ML = c()
BL = c()
TL = c()
SL = c()
MW = c()
TW = c()
SW = c()
for (i in 1:length(SYG6_files)) {
  filename <- paste(datadir, SYG6_files[i], sep = "")
  print(filename)
  landmarks <- read.tps(filename)
  ML <- c(ML, landmark_dist(landmarks, 1, 7))
  BL <- c(BL, bl_dist(landmarks))
  TL <- c(TL, tl_dist(landmarks))
  SL <- c(SL, sl_dist(landmarks))
  MW <- c(MW, landmark_dist(landmarks, 2, 11))
  TW <- c(TW, landmark_dist(landmarks, 3, 10))
  SW <- c(SW, landmark_dist(landmarks, 5, 8))
}

#list of all attributes
df <- data.frame(ML, BL, TL, SL, MW, TW, SW, files)
#list of all attributes without ID
DF <- df[,1:7]

#simpler way to calculate Correlataion, you can use
cor(DF)

#a scatterplot matrix of Correlation
pairs(DF)

#simpler way to calculate Correlataion, you can use
cor(DF)

#a scatterplot matrix of Correlation
pairs(DF)

# CV%
sd(ML)/mean(ML)*100
sd(BL)/mean(BL)*100
sd(TL)/mean(TL)*100
sd(SL)/mean(SL)*100
sd(MW)/mean(MW)*100
sd(TW)/mean(TW)*100
sd(SW)/mean(SW)*100

#CV ranges are 20-33% more broad...
```





I'm working on how to read oulines and how to use them for anlaysis

```{r}
# reading landmarks and outline in tps, code from Morpho package, adjust readallTPS function. 
readalltps = function(file) 
{
    out = list()
    exOut <- FALSE
    noutline <- 0
    input <- readLines(file)
    LM <- grep("LM=", input)
    LMstring <- input[LM]
    nLM <- sapply(LMstring, strsplit, split = "=")
    nLM <- unlist(nLM)
    nLM <- as.integer(nLM[-which(nLM == "LM")])
    nobs <- length(nLM)
    ID <- grep("ID=", input)
    IDstring <- input[ID]
    nID <- sapply(IDstring, function(x) {
        x <- gsub("=", "_", x)
    })
    nID <- gsub(" ", "", unlist(nID))
    out$ID <- nID
    outline <- grep("CURVES=", input)
    if (length(outline) > 0) {
        exOut <- TRUE
        outlinestring <- input[outline]
        noutline <- sapply(outlinestring, strsplit, split = "=")
        noutline <- unlist(noutline)
        noutline <- as.integer(noutline[-which(noutline == "CURVES")])
        LMoutline <- (sapply(outline, function(x) {
            x <- max(which(LM < x))
        }))
    }
    LMdata <- list()
    for (i in 1:nobs) {
        if (nLM[i] > 0) {
            LMdata[[i]] <- as.numeric(unlist(strsplit(unlist(input[c((LM[i] + 
                1):(LM[i] + nLM[i]))]), split = " ")))
            LMdata[[i]] <- matrix(LMdata[[i]], nLM[i], 2, byrow = TRUE)
        }
        else LMdata[[i]] <- NA
    }
    names(LMdata) <- nID
    out$LM <- LMdata
    if (exOut) {
        outlineData <- list()
        for (i in 1:nobs) {
            if (i %in% LMoutline) {
                i1 <- grep(i, LMoutline)
                outlinetmp <- list()
                ptr <- outline[i1] + 1
                j <- 1
                while (j <= noutline[i1]) {
                  tmpnr <- as.integer(unlist(strsplit(input[ptr], 
                    split = "="))[2])
                  outlinetmp[[j]] <- matrix(as.numeric(unlist(strsplit(input[(ptr + 
                    1):(ptr + tmpnr)], split = " "))), tmpnr, 
                    2, byrow = TRUE)
                  ptr <- ptr + tmpnr + 1
                  j <- j + 1
                }
                outlineData[[i]] <- outlinetmp
            }
            else outlineData[[i]] <- NA
        }
        names(outlineData) <- nID
        out$outlines <- outlineData
    }
    cat(paste("Read", nobs, "datasets with", sum(noutline), "outlines\n"))
    return(out)
}

#try to read all data(landmarks and outlines) from one file
lm_trial <- readalltps(here("analysis/data/raw_data/A_BG_2.tps"))

#read all files from raw_data
tps_files <- list.files(here("analysis/data/raw_data"), full.names = TRUE)
lm_landmarks <- map(tps_files, readalltps)
names(lm_landmarks) <- list.files(here("analysis/data/raw_data"))


lm_landmarks = c()
for (i in 1:length(files)) {
  filename <- paste(datadir, files[i], sep = "")
  print(filename)
  landmarks <- readalltps(filename)
 lm_landmarks <- c(lm_landmarks, landmarks)
}

```




# Results

```{r get-data, eval = FALSE}
# Note the path that we need to use to access our data files when rendering this document
my_data <- read.csv(here::here('analysis', 'data', 'raw_data', 'my_csv_file.csv'))
```

# Discussion

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break when the output is MS Word. For page breaks in PDF, use \newpage on its own line.  -->
##### pagebreak

# References 
<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->
<div id="refs"></div>

##### pagebreak

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
git2r::repository(here::here())
```
